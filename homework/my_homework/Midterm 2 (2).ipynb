{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "559895d2",
   "metadata": {},
   "source": [
    "# Midterm 2\n",
    "\n",
    "## FINM 36700 - 2024\n",
    "\n",
    "### UChicago Financial Mathematics\n",
    "\n",
    "* Mark Hendricks\n",
    "* hendricks@uchicago.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8f46f2",
   "metadata": {},
   "source": [
    "* Name: Jose Luna \n",
    "* Disclaimer: I used my own code and also utilized GPT and Copilot for syntax and editing corrections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cde8d3",
   "metadata": {},
   "source": [
    "# Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc273c1a",
   "metadata": {},
   "source": [
    "## Please note the following:\n",
    "\n",
    "Points\n",
    "* The exam is 100 points.\n",
    "* You have 120 minutes to complete the exam.\n",
    "* For every minute late you submit the exam, you will lose one point.\n",
    "\n",
    "\n",
    "Submission\n",
    "* You will upload your solution to the `Midterm 2` assignment on Canvas, where you downloaded this. \n",
    "* Be sure to **submit** on Canvas, not just **save** on Canvas.\n",
    "* Your submission should be readable, (the graders can understand your answers.)\n",
    "* Your submission should **include all code used in your analysis in a file format that the code can be executed.** \n",
    "\n",
    "Rules\n",
    "* The exam is open-material, closed-communication.\n",
    "* You do not need to cite material from the course github repo - you are welcome to use the code posted there without citation.\n",
    "\n",
    "Advice\n",
    "* If you find any question to be unclear, state your interpretation and proceed. We will only answer questions of interpretation if there is a typo, error, etc.\n",
    "* The exam will be graded for partial credit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624f27b1",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "**All data files are found in the class github repo, in the `data` folder.**\n",
    "\n",
    "This exam makes use of the following data files:\n",
    "* `midterm_2_data.xlsx`\n",
    "\n",
    "This file contains the following sheets:\n",
    "- for Section 2:\n",
    "    * `sector stocks excess returns` - MONTHLY excess returns for 49 sector stocks\n",
    "    * `factors excess returns` - MONTHLY excess returns of AQR factor model from Homework 5\n",
    "- for Section 3:\n",
    "    * `factors excess returns` - MONTHLY excess returns of AQR factor model from Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf6e066",
   "metadata": {},
   "source": [
    "## Scoring\n",
    "\n",
    "| Problem | Points |\n",
    "|---------|--------|\n",
    "| 1       | 25     |\n",
    "| 2       | 40     |\n",
    "| 3       | 35     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb2fc26",
   "metadata": {},
   "source": [
    "### Each numbered question is worth 5 points unless otherwise specified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81156e8f",
   "metadata": {},
   "source": [
    "# 1. Short Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cf4bc8",
   "metadata": {},
   "source": [
    "#### No Data Needed\n",
    "\n",
    "These problems do not require any data file. Rather, analyze them conceptually. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed2ec27",
   "metadata": {},
   "source": [
    "### 1.1.\n",
    "\n",
    "Historically, which pricing factor among the ones we studied has shown a considerable decrease in importance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0600fb6",
   "metadata": {},
   "source": [
    "*Answer* : \n",
    "\n",
    "The Size Factor (`SMB`) has shown a considerable decrease in importance historically. Initially highlighted in the Fama-French Three-Factor Model, the size factor represented the return difference between small-cap and large-cap stocks. However, empirical evidence and recent research indicate that the size factor's mean excess return has diminished over time, making it less significant in explaining asset returns. This is reflected in modern quantitative strategies (for instance, tangency portfolio in MV), where the size factor often receives minimal weight or is excluded altogether due to its low or uncertain excess returns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65c8109",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 1.2.\n",
    "\n",
    "True or False: For a given factor model and a set of test assets, the addition of one more factor to that model will surely decrease the cross-sectional MAE. \n",
    "\n",
    "True or False: For a given factor model and a set of test assets, the addition of one more factor to that model will surely decrease the time-series MAE. \n",
    "\n",
    "Along with stating T/F, explain your reasoning for the two statements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2b7379",
   "metadata": {},
   "source": [
    "*Answer*: \n",
    "\n",
    "1. **True** : In cross-sectional tests, adding an additional factor introduces more parameters, which generally improves the model's fit to the data. The cross-sectional Mean Absolute Error (MAE) measures the average deviation of predicted asset returns from actual returns across different assets at a single point in time. By adding more factors, we will generally capture more of the variation in asset returns, thereby reducing the cross-sectional MAE. At least we impose contraints on the model.\n",
    "\n",
    "2. **False**: In time-series tests, adding an additional factor does not guarantee a decrease in the MAE. In time-series regressions, the focus is on the alpha (intercept term) as a proxy of MAE. Adding more factors does not necessarily reduce the magnitude of the alpha, and in some cases, it might even increase the time-series MAE if the new factor does not align well with the asset's return dynamics. But it would improve the R-Squared of the model (which is not important in TS)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c00026",
   "metadata": {},
   "source": [
    "### 1.3.\n",
    "\n",
    "Consider the scenario in which you are helping two people with investments.\n",
    "\n",
    "* The young person has a 50 year investment horizon.\n",
    "* The elderly person has a 10 year investment horizon.\n",
    "* Both individuals have the same portfolio holdings.\n",
    "\n",
    "State who has the more certain cumulative return and explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c6430f",
   "metadata": {},
   "source": [
    "*Answer* :\n",
    "\n",
    "While the average (mean) return becomes more predictable over longer horizons due to the Law of Large Numbers, the variance of cumulative returns increases with time. This is because cumulative returns compound over time, and the uncertainty (volatility) associated with returns accumulates. For **log-normal returns**, the variance of cumulative returns scales linearly with the time horizon (h). Therefore, over a 50-year period, the young person faces greater uncertainty in cumulative returns compared to the elderly person over a 10-year period. The shorter time horizon results in less cumulative volatility, making the elderly person's cumulative return more certain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460d4d72",
   "metadata": {},
   "source": [
    "### 1.4.\n",
    "\n",
    "Suppose we find that the 10-year bond yield works well as a new pricing factor, along with `MKT`.\n",
    "\n",
    "Consider two ways of building this new factor.\n",
    "1. Directly use the index of 10-year yields, `YLD`\n",
    "1. Construct a Fama-French style portfolio of equities, `FFYLD`. (Rank all the stocks by their correlation to bond yield changes, and go long the highest ranked and short the lowest ranked.)\n",
    "\n",
    "Could you test the model with `YLD` and the model with `FFYLD` in the exact same ways? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337da4bf",
   "metadata": {},
   "source": [
    "*Answer* :\n",
    "\n",
    "No, we cannot test the model with `YLD` and the model with `FFYLD` in the exact same ways. Since `YLD` is a economic factor (Non-return Factor), so we can not use Time Series, only a Cross Sectional model validation. On contrast, if we construct `FFYLD` following the FF style, it would be a return factor, and we can validate with either Time Series or Cross Sectional Validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf2d238",
   "metadata": {},
   "source": [
    "### 1.5.\n",
    "\n",
    "Suppose we implement a momentum strategy on cryptocurrencies rather than US stocks.\n",
    "\n",
    "Conceptually speaking, but specific to the context of our course discussion, how would the risk profile differ from the momentum strategy of US equities?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d637fbec",
   "metadata": {},
   "source": [
    "*Answer* : \n",
    "\n",
    "The momentum strategy on cryptocurrencies would likely entail higher overall risk due to increased volatility and possible liquidity concerns. These factors contribute to a risk profile that is different and potentially more risk-taker than that of a momentum strategy on US equities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632ce7d4",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a8a354",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 2. Pricing and Tangency Portfolio\n",
    "\n",
    "You work in a hedge fund that believes that the AQR 4-Factor Model (present in Homework 5) is the perfect pricing model for stocks.\n",
    "\n",
    "$$\n",
    "\\mathbb{E} \\left[ \\tilde{r}^i \\right] = \\beta^{i,\\text{MKT}} \\mathbb{E} \\left[ \\tilde{f}_{\\text{MKT}} \\right] + \\beta^{i,\\text{HML}} \\mathbb{E} \\left[ \\tilde{f}_{\\text{HML}} \\right] + \\beta^{i,\\text{RMW}} \\mathbb{E} \\left[ \\tilde{f}_{\\text{RMW}} \\right] + \\beta^{i,\\text{UMD}} \\mathbb{E} \\left[ \\tilde{f}_{\\text{UMD}} \\right]\n",
    "$$\n",
    "\n",
    "The factors are available in the sheet `factors excess returns`.\n",
    "\n",
    "The hedge fund invests in sector-tracking ETFs available in the sheet `sectors excess returns`. You are to allocate into these sectors according to a mean-variance optimization with...\n",
    "\n",
    "* regularization: elements outside the diagonal covariance matrix divided by 2.\n",
    "* modeled risk premia: expected excess returns given by the factor model rather than just using the historic sample averages.\n",
    "\n",
    "You are to train the portfolio and test out-of-sample. The timeframes should be:\n",
    "* Training timeframe: Jan-2018 to Dec-2022.\n",
    "* Testing timeframe: Jan-2023 to most recent observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "140425fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew,kurtosis,norm\n",
    "import statsmodels.api as sm\n",
    "\n",
    "path = \"../../data/midterm_2_data.xlsx\"\n",
    "data_sectors = pd.read_excel(path, sheet_name=\"sector excess returns\", index_col=0)\n",
    "data_factors = pd.read_excel(path, sheet_name=\"factors excess returns\", index_col=0)\n",
    "\n",
    "data_factors_training = data_factors[(data_factors.index >= \"2018-01-01\") & (data_factors.index < \"2022-12-31\")]\n",
    "data_sectors_training = data_sectors[(data_sectors.index >= \"2018-01-01\") & (data_sectors.index < \"2022-12-31\")]\n",
    "data_sectors_testing = data_sectors[(data_sectors.index >= \"2023-01-01\") ]\n",
    "data_factors_testing = data_factors[(data_factors.index >= \"2023-01-01\") ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72c4f8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_tag_reg(return_db, adj_factor = 12, diagonal_factor = 1, denominator = 2):\n",
    "    sigma = (return_db.cov()*adj_factor)\n",
    "    sigma_reg = (sigma + diagonal_factor*np.diag(np.diag(sigma)))/denominator\n",
    "    mu_excess = (return_db.mean()*adj_factor)\n",
    "    vector = np.ones(len(mu_excess))\n",
    "    w_tan = (np.linalg.inv(sigma_reg) @ mu_excess )/(np.transpose(vector) @ np.linalg.inv(sigma_reg) @ mu_excess)\n",
    "    weights_db = pd.DataFrame({\"w_tan_reg\": w_tan})\n",
    "    weights_db.index = return_db.columns\n",
    "    return weights_db\n",
    "def get_metric_returns(returns, weights=[], adj_factor=12, VaR_q=5):\n",
    "\n",
    "\n",
    "    # If weights are provided, compute the weighted returns\n",
    "    if len(weights) == 0:\n",
    "        port_metrics = returns.copy()\n",
    "    else:\n",
    "        port_metrics = returns @ weights\n",
    "        port_metrics = pd.DataFrame(port_metrics, columns=['Portfolio'])\n",
    "\n",
    "    # Initialize the result DataFrame\n",
    "    result = pd.DataFrame()\n",
    "\n",
    "    # Compute Mean, Volatility, Sharpe Ratio, Skew, Excess Kurtosis\n",
    "    if len(weights) == 0:\n",
    "        port_metrics_r = pd.DataFrame({\n",
    "            \"Mean\": port_metrics.mean() * adj_factor,\n",
    "            \"Volatility\": port_metrics.std() * np.sqrt(adj_factor)\n",
    "        })\n",
    "        port_metrics_r[\"Sharpe_Ratio\"] = (port_metrics.mean() / port_metrics.std()) * np.sqrt(adj_factor)\n",
    "        port_metrics_r[\"Skew\"] = port_metrics.apply(skew)\n",
    "        port_metrics_r[\"Excess Kurtosis\"] = port_metrics.apply(kurtosis, fisher=True, bias=False)\n",
    "    else:\n",
    "        asset = 'Portfolio'\n",
    "        port_metrics_r = pd.DataFrame({\n",
    "            \"Mean\": [port_metrics[asset].mean() * adj_factor],\n",
    "            \"Volatility\": [port_metrics[asset].std() * np.sqrt(adj_factor)]\n",
    "        }, index=[asset])\n",
    "        port_metrics_r[\"Sharpe_Ratio\"] = (port_metrics[asset].mean() / port_metrics[asset].std()) * np.sqrt(adj_factor)\n",
    "        port_metrics_r[\"Skew\"] = skew(port_metrics[asset])\n",
    "        port_metrics_r[\"Excess Kurtosis\"] = kurtosis(port_metrics[asset], fisher=True, bias=False)\n",
    "\n",
    "    # Compute VaR, CVaR, Max Drawdown, and related metrics\n",
    "    for asset in port_metrics.columns:\n",
    "        data_aux = port_metrics[[asset]].copy()\n",
    "        VaR = np.percentile(sorted(data_aux.values.flatten()), q=VaR_q)\n",
    "        CVaR = data_aux[data_aux[asset] <= VaR].mean().values[0]\n",
    "\n",
    "        data_aux_acum_return = (data_aux + 1).cumprod()\n",
    "        data_aux_max_cum_return = data_aux_acum_return.cummax()\n",
    "        data_aux_drawdown = ((data_aux_acum_return - data_aux_max_cum_return) / data_aux_max_cum_return)\n",
    "        max_drawdown = data_aux_drawdown.min().values[0]\n",
    "        max_drawdown_date = data_aux_drawdown.idxmin().values[0]\n",
    "        peak_idx = data_aux_max_cum_return.idxmax().values[0]\n",
    "        recovery_data = data_aux_drawdown[data_aux_drawdown.index >= max_drawdown_date]\n",
    "        recovery_idx = recovery_data[recovery_data[asset] >= -0.00001].first_valid_index()\n",
    "        duration = (recovery_idx - max_drawdown_date).days if recovery_idx else np.nan\n",
    "\n",
    "        aux_result = pd.DataFrame(\n",
    "            [[VaR, CVaR, max_drawdown, max_drawdown_date, peak_idx, recovery_idx, duration]],\n",
    "            columns=[\"VaR\", \"CVaR\", \"Max Drawdown\", \"Bottom\", \"Peak\", \"Recovery\", \"Duration (days)\"],\n",
    "            index=[asset]\n",
    "        )\n",
    "        result = pd.concat([result, aux_result], axis=0)\n",
    "\n",
    "    # Merge the two sets of metrics\n",
    "    metrics = pd.merge(port_metrics_r, result, left_index=True, right_index=True, how=\"left\")\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db465bc9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 2.1.\n",
    "(8pts)\n",
    "\n",
    "Calculate the model-implied expected excess returns of every asset.\n",
    "\n",
    "The time-series estimations should...\n",
    "* NOT include an intercept. (You assume the model holds perfectly.)\n",
    "* use data from the `training` timeframe.\n",
    "\n",
    "With the time-series estimates, use the `training` timeframe's sample average of the factors as the factor premia. Together, this will give you the model-implied risk premia, which we label as\n",
    "$$\n",
    "\\lambda_i := \\mathbb{E}[\\tilde{r}_i]\n",
    "$$\n",
    "\n",
    "* Store $\\lambda_i$ and $\\boldsymbol{\\beta}^i$ for each asset.\n",
    "* Print $\\lambda_i$ for `Agric`, `Food`, `Soda`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f51b1bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "\n",
    "for portfolio in data_sectors_training.columns:\n",
    "    X = data_factors_training.copy()\n",
    "    y = data_sectors_training[portfolio].copy()\n",
    "    mod = sm.OLS(y, X).fit()\n",
    "    aux = pd.DataFrame(mod.params).T\n",
    "    aux.index = [portfolio]\n",
    "    aux[\"R_squared\"] = mod.rsquared\n",
    "    results = pd.concat([results,aux],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4ee16a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MKT</th>\n",
       "      <th>HML</th>\n",
       "      <th>RMW</th>\n",
       "      <th>UMD</th>\n",
       "      <th>R_squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Agric</th>\n",
       "      <td>0.832362</td>\n",
       "      <td>0.556541</td>\n",
       "      <td>-0.502093</td>\n",
       "      <td>0.038972</td>\n",
       "      <td>0.544657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Food</th>\n",
       "      <td>0.524509</td>\n",
       "      <td>0.205452</td>\n",
       "      <td>0.309711</td>\n",
       "      <td>-0.003572</td>\n",
       "      <td>0.567740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soda</th>\n",
       "      <td>0.540240</td>\n",
       "      <td>0.179127</td>\n",
       "      <td>0.638443</td>\n",
       "      <td>0.013688</td>\n",
       "      <td>0.536068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beer</th>\n",
       "      <td>0.541267</td>\n",
       "      <td>0.022959</td>\n",
       "      <td>0.629678</td>\n",
       "      <td>-0.040541</td>\n",
       "      <td>0.613413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Smoke</th>\n",
       "      <td>0.498185</td>\n",
       "      <td>0.443078</td>\n",
       "      <td>0.402220</td>\n",
       "      <td>-0.134015</td>\n",
       "      <td>0.391324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            MKT       HML       RMW       UMD  R_squared\n",
       "Agric  0.832362  0.556541 -0.502093  0.038972   0.544657\n",
       "Food   0.524509  0.205452  0.309711 -0.003572   0.567740\n",
       "Soda   0.540240  0.179127  0.638443  0.013688   0.536068\n",
       "Beer   0.541267  0.022959  0.629678 -0.040541   0.613413\n",
       "Smoke  0.498185  0.443078  0.402220 -0.134015   0.391324"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6d6dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Expected Return TS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Agric</th>\n",
       "      <td>0.005415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Food</th>\n",
       "      <td>0.005410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soda</th>\n",
       "      <td>0.006847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Expected Return TS\n",
       "Agric            0.005415\n",
       "Food             0.005410\n",
       "Soda             0.006847"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "premium_TS = results[['MKT', 'HML', 'RMW', 'UMD']] @ pd.DataFrame(data_factors_training.mean()*12a)\n",
    "premium_TS.columns = [\"Expected Return TS\"]\n",
    "premium_TS.loc[[ \"Agric\", \"Food \", \"Soda \"],:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b80c6b5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 2.2.\n",
    "\n",
    "Use the expected excess returns derived from (2.1) with the **regularized** covariance matrix to calculate the weights of the tangency portfolio.\n",
    "\n",
    "- Use the covariance matrix only for `training` timeframe.\n",
    "- Calculate and store the vector of weights for all the assets.\n",
    "- Return the weights of the tangency portfolio for `Agric`, `Food`, `Soda`.\n",
    "\n",
    "$$\n",
    "\\textbf{w}_{t} = \\dfrac{\\tilde{\\Sigma}^{-1} \\bm{\\lambda}}{\\bm{1}' \\tilde{\\Sigma}^{-1} \\bm{\\lambda}}\n",
    "$$\n",
    "\n",
    "Where $\\tilde{\\Sigma}^{-1}$ is the regularized covariance-matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e4ef8dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_tan_reg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Agric</th>\n",
       "      <td>-0.030723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Food</th>\n",
       "      <td>0.015320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soda</th>\n",
       "      <td>0.132944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       w_tan_reg\n",
       "Agric  -0.030723\n",
       "Food    0.015320\n",
       "Soda    0.132944"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma = data_sectors_training.cov()*12\n",
    "sigma_reg = (sigma + np.diag(np.diag(sigma)))/2\n",
    "mu_excess = premium_TS\n",
    "vector = np.ones(len(mu_excess))\n",
    "w_tan = (np.linalg.inv(sigma_reg) @ mu_excess )/(np.transpose(vector) @ np.linalg.inv(sigma_reg) @ mu_excess)\n",
    "weights_db = pd.DataFrame(w_tan)\n",
    "weights_db.index = data_sectors_training.columns\n",
    "weights_db.columns = [\"w_tan_reg\"]\n",
    "weights_db.loc[[ \"Agric\", \"Food \", \"Soda \"],:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5c171c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 2.3.\n",
    "\n",
    "Evaluate the performance of this allocation in the `testing` period. Report the **annualized**\n",
    "- mean\n",
    "- vol\n",
    "- Sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4f48a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_f2b49\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f2b49_level0_col0\" class=\"col_heading level0 col0\" >Mean</th>\n",
       "      <th id=\"T_f2b49_level0_col1\" class=\"col_heading level0 col1\" >Volatility</th>\n",
       "      <th id=\"T_f2b49_level0_col2\" class=\"col_heading level0 col2\" >Sharpe_Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f2b49_level0_row0\" class=\"row_heading level0 row0\" >w_tan_reg</th>\n",
       "      <td id=\"T_f2b49_row0_col0\" class=\"data row0 col0\" >18.12%</td>\n",
       "      <td id=\"T_f2b49_row0_col1\" class=\"data row0 col1\" >11.95%</td>\n",
       "      <td id=\"T_f2b49_row0_col2\" class=\"data row0 col2\" >151.55%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x26da5d8b710>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metric_returns(data_sectors_testing @ weights_db, adj_factor=12, VaR_q=5)[[\"Mean\", \"Volatility\", \"Sharpe_Ratio\"]].style.format(\"{:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a6f8bc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 2.4.\n",
    "\n",
    "(7pts)\n",
    "\n",
    "Construct the same tangency portfolio as in `2.2` but with one change:\n",
    "* replace the risk premia of the assets, (denoted $\\lambda_i$) with the sample averages of the excess returns from the `training` set.\n",
    "\n",
    "So instead of using $\\lambda_i$ suggested by the factor model (as in `2.1-2.3`) you're using sample averages for $\\lambda_i$.\n",
    "\n",
    "- Return the weights of the tangency portfolio for `Agric`, `Food`, `Soda`.\n",
    "\n",
    "Evaluate the performance of this allocation in the `testing` period. Report the **annualized**\n",
    "- mean\n",
    "- vol\n",
    "- Sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ab55275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_tan_reg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Agric</th>\n",
       "      <td>0.14409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Food</th>\n",
       "      <td>-0.06981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soda</th>\n",
       "      <td>0.32268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       w_tan_reg\n",
       "Agric    0.14409\n",
       "Food    -0.06981\n",
       "Soda     0.32268"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma = data_sectors_training.cov()*12\n",
    "sigma_reg = (sigma + np.diag(np.diag(sigma)))/2\n",
    "mu_excess = data_sectors_training.mean()*12\n",
    "vector = np.ones(len(mu_excess))\n",
    "w_tan = (np.linalg.inv(sigma_reg) @ mu_excess )/(np.transpose(vector) @ np.linalg.inv(sigma_reg) @ mu_excess)\n",
    "weights_db_sample_mean = pd.DataFrame(w_tan)\n",
    "weights_db_sample_mean.index = data_sectors_training.columns\n",
    "weights_db_sample_mean.columns = [\"w_tan_reg\"]\n",
    "weights_db_sample_mean.loc[[ \"Agric\", \"Food \", \"Soda \"],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d816882d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_04a1a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_04a1a_level0_col0\" class=\"col_heading level0 col0\" >Mean</th>\n",
       "      <th id=\"T_04a1a_level0_col1\" class=\"col_heading level0 col1\" >Volatility</th>\n",
       "      <th id=\"T_04a1a_level0_col2\" class=\"col_heading level0 col2\" >Sharpe_Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_04a1a_level0_row0\" class=\"row_heading level0 row0\" >w_tan_reg</th>\n",
       "      <td id=\"T_04a1a_row0_col0\" class=\"data row0 col0\" >17.68%</td>\n",
       "      <td id=\"T_04a1a_row0_col1\" class=\"data row0 col1\" >15.30%</td>\n",
       "      <td id=\"T_04a1a_row0_col2\" class=\"data row0 col2\" >115.55%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x26da6b63110>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metric_returns(data_sectors_testing @ weights_db_sample_mean, adj_factor=12, VaR_q=5)[[\"Mean\", \"Volatility\", \"Sharpe_Ratio\"]].style.format(\"{:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c172cbe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 2.5.\n",
    "\n",
    "Which allocation performed better in the `testing` period: the allocation based on premia from the factor model or from the sample averages?\n",
    "\n",
    "Why might this be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750798e6",
   "metadata": {},
   "source": [
    "*Answer* : \n",
    "\n",
    "The allocation based on premia from the factor model achieved a mean return of 17.56% with a Sharpe Ratio of 156.52%. By comparison, the allocation using the sample mean of the training data produced a similar mean return of 17.68% but with a lower Sharpe Ratio of 115.55%. This indicates that, while both methods yielded comparable average returns, the allocation based on sample averages resulted in higher out-of-sample volatility.\n",
    "\n",
    "This difference may stem from the nature of the factor model-based allocation, which aims to estimate expected returns more accurately by leveraging relationships identified in the factor model. Assuming the model accurately captures underlying return dynamics, its predictions should theoretically be a more robust estimator for out-of-sample returns than the simple historical average. In contrast, relying solely on sample averages can be prone to higher volatility and may not account for underlying drivers of returns, which can affect out-of-sample performance stability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a442fc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 2.6.\n",
    "Suppose you now want to build a tangency portfolio solely from the factors, without using the sector ETFs.\n",
    "\n",
    "- Calculate the weights of the tangency portfolio using `training` data for the factors.\n",
    "- Again, regularize the covariance matrix of factor returns by dividing off-diagonal elements by 2.\n",
    "\n",
    "Report, in the `testing` period, the factor-based tangency stats **annualized**...\n",
    "- mean\n",
    "- vol\n",
    "- Sharpe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "180077a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = data_factors_training.cov()*12\n",
    "sigma_reg = (sigma + np.diag(np.diag(sigma)))/2\n",
    "mu_excess = data_factors_training.mean()*12\n",
    "vector = np.ones(len(mu_excess))\n",
    "w_tan = (np.linalg.inv(sigma_reg) @ mu_excess )/(np.transpose(vector) @ np.linalg.inv(sigma_reg) @ mu_excess)\n",
    "weights_db_factor_training = pd.DataFrame(w_tan)\n",
    "weights_db_factor_training.index = data_factors_training.columns\n",
    "weights_db_factor_training.columns = [\"w_tan_reg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d4f0ddd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_tan_reg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MKT</th>\n",
       "      <td>0.176921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HML</th>\n",
       "      <td>-0.016221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMW</th>\n",
       "      <td>0.598427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UMD</th>\n",
       "      <td>0.240872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     w_tan_reg\n",
       "MKT   0.176921\n",
       "HML  -0.016221\n",
       "RMW   0.598427\n",
       "UMD   0.240872"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_db_factor_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1f0e2113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_669bd\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_669bd_level0_col0\" class=\"col_heading level0 col0\" >Mean</th>\n",
       "      <th id=\"T_669bd_level0_col1\" class=\"col_heading level0 col1\" >Volatility</th>\n",
       "      <th id=\"T_669bd_level0_col2\" class=\"col_heading level0 col2\" >Sharpe_Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_669bd_level0_row0\" class=\"row_heading level0 row0\" >w_tan_reg</th>\n",
       "      <td id=\"T_669bd_row0_col0\" class=\"data row0 col0\" >6.24%</td>\n",
       "      <td id=\"T_669bd_row0_col1\" class=\"data row0 col1\" >5.82%</td>\n",
       "      <td id=\"T_669bd_row0_col2\" class=\"data row0 col2\" >107.19%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x194a19c6ea0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metric_returns(data_factors_testing @ weights_db_factor_training, adj_factor=12, VaR_q=5)[[\"Mean\", \"Volatility\", \"Sharpe_Ratio\"]].style.format(\"{:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d6cc36",
   "metadata": {},
   "source": [
    "### 2.7.\n",
    "\n",
    "Based on the hedge fund's beliefs, would you prefer to use the ETF-based tangency or the factor-based tangency portfolio? Explain your reasoning. Note that you should answer based on broad principles and not on the particular estimation results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea4970a",
   "metadata": {},
   "source": [
    "*Answer* :\n",
    "\n",
    "Based on the hedge fund's beliefs, I would prefer to use the factor-based tangency portfolio. Because it should be more stable in terms that give a better diversification, a factor-based tangency portfolio focuses on underlying risk factors (e.g., market, value, momentum) rather than specific assets. This approach can provide better diversification and robustness, as factors capture common sources of risk that drive asset returns in comparison to use many ETFs for the tangency portfolio which could led to inestability of the covariance matrix (ie. the weights of the portfolio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8eda25",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ff849e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 3. Long-Run Returns\n",
    "\n",
    "For this question, use only the sheet `factors excess returns`.\n",
    "\n",
    "Suppose we want to measure the long run returns of various pricing factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078eb087",
   "metadata": {},
   "source": [
    "### 3.1.\n",
    "\n",
    "Turn the data into log returns.\n",
    "- Display the first 5 rows of the data.\n",
    "\n",
    "Using these log returns, report the **annualized**\n",
    "* mean\n",
    "* vol\n",
    "* Sharpe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "935fde46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MKT</th>\n",
       "      <th>HML</th>\n",
       "      <th>RMW</th>\n",
       "      <th>UMD</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1980-01-01</th>\n",
       "      <td>0.053636</td>\n",
       "      <td>0.017349</td>\n",
       "      <td>-0.017146</td>\n",
       "      <td>0.072786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-02-01</th>\n",
       "      <td>-0.012275</td>\n",
       "      <td>0.006081</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.075849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-03-01</th>\n",
       "      <td>-0.138113</td>\n",
       "      <td>-0.010151</td>\n",
       "      <td>0.014494</td>\n",
       "      <td>-0.100373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-04-01</th>\n",
       "      <td>0.038932</td>\n",
       "      <td>0.010544</td>\n",
       "      <td>-0.021224</td>\n",
       "      <td>-0.004309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-05-01</th>\n",
       "      <td>0.051263</td>\n",
       "      <td>0.003793</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>-0.011263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 MKT       HML       RMW       UMD\n",
       "date                                              \n",
       "1980-01-01  0.053636  0.017349 -0.017146  0.072786\n",
       "1980-02-01 -0.012275  0.006081  0.000400  0.075849\n",
       "1980-03-01 -0.138113 -0.010151  0.014494 -0.100373\n",
       "1980-04-01  0.038932  0.010544 -0.021224 -0.004309\n",
       "1980-05-01  0.051263  0.003793  0.003394 -0.011263"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_factors_log = np.log(1+ data_factors)\n",
    "data_factors_log.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ba2869e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josel\\AppData\\Local\\Temp\\ipykernel_22068\\1304460572.py:63: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  result = pd.concat([result, aux_result], axis=0)\n",
      "C:\\Users\\josel\\AppData\\Local\\Temp\\ipykernel_22068\\1304460572.py:63: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  result = pd.concat([result, aux_result], axis=0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_a2395\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a2395_level0_col0\" class=\"col_heading level0 col0\" >Mean</th>\n",
       "      <th id=\"T_a2395_level0_col1\" class=\"col_heading level0 col1\" >Volatility</th>\n",
       "      <th id=\"T_a2395_level0_col2\" class=\"col_heading level0 col2\" >Sharpe_Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a2395_level0_row0\" class=\"row_heading level0 row0\" >MKT</th>\n",
       "      <td id=\"T_a2395_row0_col0\" class=\"data row0 col0\" >7.35%</td>\n",
       "      <td id=\"T_a2395_row0_col1\" class=\"data row0 col1\" >15.88%</td>\n",
       "      <td id=\"T_a2395_row0_col2\" class=\"data row0 col2\" >46.30%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a2395_level0_row1\" class=\"row_heading level0 row1\" >HML</th>\n",
       "      <td id=\"T_a2395_row1_col0\" class=\"data row1 col0\" >1.98%</td>\n",
       "      <td id=\"T_a2395_row1_col1\" class=\"data row1 col1\" >10.98%</td>\n",
       "      <td id=\"T_a2395_row1_col2\" class=\"data row1 col2\" >18.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a2395_level0_row2\" class=\"row_heading level0 row2\" >RMW</th>\n",
       "      <td id=\"T_a2395_row2_col0\" class=\"data row2 col0\" >4.35%</td>\n",
       "      <td id=\"T_a2395_row2_col1\" class=\"data row2 col1\" >8.36%</td>\n",
       "      <td id=\"T_a2395_row2_col2\" class=\"data row2 col2\" >52.10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a2395_level0_row3\" class=\"row_heading level0 row3\" >UMD</th>\n",
       "      <td id=\"T_a2395_row3_col0\" class=\"data row3 col0\" >5.01%</td>\n",
       "      <td id=\"T_a2395_row3_col1\" class=\"data row3 col1\" >16.04%</td>\n",
       "      <td id=\"T_a2395_row3_col2\" class=\"data row3 col2\" >31.22%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x194a19c4ce0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_1y_log_return = get_metric_returns(data_factors_log, adj_factor=12, VaR_q=5)[[\"Mean\", \"Volatility\", \"Sharpe_Ratio\"]]\n",
    "metric_1y_log_return.style.format(\"{:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be343b9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 3.2.\n",
    "\n",
    "Consider 15-year cumulative log excess returns. Following the assumptions and modeling of Lecture 6, report the following 15-year stats:\n",
    "- mean\n",
    "- vol\n",
    "- Sharpe\n",
    "\n",
    "How do they compare to the estimated stats (1-year horizon) in `3.1`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c232212f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_61112\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_61112_level0_col0\" class=\"col_heading level0 col0\" >Mean</th>\n",
       "      <th id=\"T_61112_level0_col1\" class=\"col_heading level0 col1\" >Volatility</th>\n",
       "      <th id=\"T_61112_level0_col2\" class=\"col_heading level0 col2\" >Sharpe_Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_61112_level0_row0\" class=\"row_heading level0 row0\" >MKT</th>\n",
       "      <td id=\"T_61112_row0_col0\" class=\"data row0 col0\" >110.32%</td>\n",
       "      <td id=\"T_61112_row0_col1\" class=\"data row0 col1\" >61.52%</td>\n",
       "      <td id=\"T_61112_row0_col2\" class=\"data row0 col2\" >179.33%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61112_level0_row1\" class=\"row_heading level0 row1\" >HML</th>\n",
       "      <td id=\"T_61112_row1_col0\" class=\"data row1 col0\" >29.65%</td>\n",
       "      <td id=\"T_61112_row1_col1\" class=\"data row1 col1\" >42.52%</td>\n",
       "      <td id=\"T_61112_row1_col2\" class=\"data row1 col2\" >69.75%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61112_level0_row2\" class=\"row_heading level0 row2\" >RMW</th>\n",
       "      <td id=\"T_61112_row2_col0\" class=\"data row2 col0\" >65.31%</td>\n",
       "      <td id=\"T_61112_row2_col1\" class=\"data row2 col1\" >32.37%</td>\n",
       "      <td id=\"T_61112_row2_col2\" class=\"data row2 col2\" >201.77%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61112_level0_row3\" class=\"row_heading level0 row3\" >UMD</th>\n",
       "      <td id=\"T_61112_row3_col0\" class=\"data row3 col0\" >75.14%</td>\n",
       "      <td id=\"T_61112_row3_col1\" class=\"data row3 col1\" >62.14%</td>\n",
       "      <td id=\"T_61112_row3_col2\" class=\"data row3 col2\" >120.93%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x194a19c78c0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_15y_log_return = metric_1y_log_return.copy()\n",
    "metric_15y_log_return[\"Mean\"] = metric_15y_log_return[\"Mean\"]*15\n",
    "metric_15y_log_return[\"Volatility\"] = metric_15y_log_return[\"Volatility\"]*np.sqrt(15)\n",
    "metric_15y_log_return[\"Sharpe_Ratio\"] = metric_15y_log_return[\"Mean\"]/metric_15y_log_return[\"Volatility\"]\n",
    "metric_15y_log_return.style.format(\"{:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20b5f27",
   "metadata": {},
   "source": [
    "*Answer:*\n",
    "\n",
    "Based on the assumptions presented in Lecture 6, expected returns over multiple periods increase proportionally with time. Specifically, over 15 years, the expected return is 15 times the one-year expected return. Meanwhile, the variance of returns increases linearly with time, meaning the standard deviation (the square root of variance) increases with the square root of time. For a 15-year period, the standard deviation is therefore multiplied by 15 compared to the one-year standard deviation.\n",
    "\n",
    "As a result, the Sharpe Ratio over 15 yearswhich is calculated as the expected return divided by the standard deviationbecomes the one-year Sharpe Ratio multiplied by 15. This is because the numerator (expected return) is multiplied by 15, while the denominator (standard deviation) is multiplied by 15, leading to an overall multiplication of the Sharpe Ratio by 15."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3181ba0c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 3.3.\n",
    "\n",
    "What is the probability that momentum factor has a negative mean excess return over the next \n",
    "* single period?\n",
    "* 15 years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6f14bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = data_factors_log[\"UMD\"].mean() * 12, data_factors_log[\"UMD\"].std() * np.sqrt(12)\n",
    "prob_1 = norm.cdf(0, mu, sigma/np.sqrt(1))\n",
    "prob_15 = norm.cdf(0, mu, sigma/np.sqrt(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ee33e3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of negative return in 1 year: 37.74%\n",
      "Probability of negative return in 15 years: 11.33%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Probability of negative return in 1 year: {prob_1:.2%}\")\n",
    "print(f\"Probability of negative return in 15 years: {prob_15:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a137b86c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 3.4.\n",
    "\n",
    "Recall from the case that momentum has been underperforming since 2009. \n",
    "\n",
    "Using data from 2009 to present, what is the probability that momentum *outperforms* the market factor over the next\n",
    "* period?\n",
    "* 15 years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "017ca964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of positive spread (momentum outperforms the market) in 1 year: 28.39%\n",
      "Probability of positive spread (momentum outperforms the market) in 15 years: 1.35%\n"
     ]
    }
   ],
   "source": [
    "data_factors_log_present = data_factors_log[data_factors_log.index >= \"2009-01-01\"]\n",
    "spread = data_factors_log_present['UMD'] - data_factors_log_present['MKT']\n",
    "mu, sigma = spread.mean() * 12, spread.std() * np.sqrt(12)\n",
    "\n",
    "prob_1y = 1 - norm.cdf(0, mu, sigma/np.sqrt(1))\n",
    "prob_15y = 1 - norm.cdf(0, mu, sigma/np.sqrt(15))\n",
    "\n",
    "print(f\"Probability of positive spread (momentum outperforms the market) in 1 year: {prob_1y:.2%}\")\n",
    "print(f\"Probability of positive spread (momentum outperforms the market) in 15 years: {prob_15y:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5678bc07",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 3.5.\n",
    "Conceptually, why is there such a discrepancy between this probability for 1 period vs. 15 years?\n",
    "\n",
    "What assumption about the log-returns are we making when we use this technique to estimate underperformance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639dedad",
   "metadata": {},
   "source": [
    "*Answer*:\n",
    "\n",
    "The discrepancy arises because, under the assumption that log-returns are normally distributed and independent, cumulative log-returns over multiple periods add up linearly, while their standard deviation increases with the square root of time; since the mean spread between Momentum and the Market post-2009 is negative, extending the time horizon amplifies the negative expected return more rapidly than the growth in standard deviation, leading to a much lower probability of outperformance over 15 years (1.35%) compared to 1 year (28.39%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868f33b7",
   "metadata": {},
   "source": [
    "### 3.6.\n",
    "\n",
    "Using your previous answers, explain what is meant by time diversification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3512bb3",
   "metadata": {},
   "source": [
    "*Answer*:\n",
    "\n",
    "Time diversification means that investing over a longer time horizon can reduce the impact of volatility on the average return due to the Law of Large Numbers; with independent, positive expected returns, the standard deviation of the average return decreases with the square root of time, enhancing the Sharpe Ratio and lowering the risk of underperformance, but in this case, because the expected return (mean spread between Momentum and the Market) is negative, extending the time horizon actually increases the cumulative negative return and the probability of underperformance, illustrating that time diversification can amplify risk when expected returns are negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5080207",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 3.7.\n",
    "\n",
    "Is the probability that `HML` and `UMD` both have negative cumulative returns over the next year higher or lower than the probability that `HML` and `MKT` both have negative cumulative returns over the next year?\n",
    "\n",
    "Answer conceptually, but specifically. (No need to calculate the specific probabilities.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bf007a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of negative return in 1 year for HML and UMD: 16.17%\n",
      "Probability of negative return in 1 year for HML and MKT: 13.79%\n"
     ]
    }
   ],
   "source": [
    "mu, sigma = data_factors_log[\"HML\"].mean() * 12, data_factors_log[\"HML\"].std() * np.sqrt(12)\n",
    "prob_1_hml = norm.cdf(0, mu, sigma/np.sqrt(1))\n",
    "\n",
    "mu, sigma = data_factors_log[\"UMD\"].mean() * 12, data_factors_log[\"UMD\"].std() * np.sqrt(12)\n",
    "prob_1_umd = norm.cdf(0, mu, sigma/np.sqrt(1))\n",
    "\n",
    "mu, sigma = data_factors_log[\"MKT\"].mean() * 12, data_factors_log[\"MKT\"].std() * np.sqrt(12)\n",
    "prob_1_mkt = norm.cdf(0, mu, sigma/np.sqrt(1))\n",
    "\n",
    "print(f\"Probability of negative return in 1 year for HML and UMD: {prob_1_hml * prob_1_umd:.2%}\")\n",
    "print(f\"Probability of negative return in 1 year for HML and MKT: {prob_1_hml * prob_1_mkt:.2%}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0ea54d",
   "metadata": {},
   "source": [
    "*Answer* : \n",
    "\n",
    "The probability that both `HML` (Value factor) and `UMD` (Momentum factor) have negative cumulative returns over the next year is **higher** than the probability that both `HML` and `MKT` (Market) do, because `UMD` has a higher individual probability of negative returns than MKT (momentum has a lower mean and higher variance) so when considering their joint probabilities (assuming independence), the combination of `HML` and `UMD` results in a higher likelihood of both being negative compared to the combination of `HML` and `MKT`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cf51ec",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
